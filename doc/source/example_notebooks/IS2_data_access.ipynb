{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing ICESat-2 Data\n",
    "This notebook ({nb-download}`download <IS2_data_access.ipynb>`) illustrates the use of icepyx for programmatic ICESat-2 data query and download from the NASA National Snow and Ice Data Center Distributed Active Archive Center (NASA NSIDC DAAC).\n",
    "A complimentary notebook demonstrates in greater detail the [subsetting](https://icepyx.readthedocs.io/en/latest/example_notebooks/IS2_data_access2-subsetting.html) options available when ordering data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages, including icepyx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import icepyx as ipx\n",
    "import os\n",
    "import shutil\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "---------------------------------\n",
    "\n",
    "## Quick-Start Guide\n",
    "\n",
    "The entire process of getting ICESat-2 data (from query to download) can ultimately be accomplished in two minimal lines of code:\n",
    "\n",
    "`region_a = ipx.Query(short_name, spatial_extent, date_range)`\n",
    "\n",
    "`region_a.download_granules(path)`\n",
    "\n",
    "where the function inputs are described in more detail below.\n",
    "\n",
    "**The rest of this notebook explains the required inputs used above, optional inputs not available in the minimal example, and the other data search and visualization tools built in to icepyx that make it easier for the user to find, explore, and download ICESat-2 data programmatically from the NASA NSIDC DAAC.** The detailed steps outlined and the methods showcased below are meant to give the user more control over the data they find and download (including options to order/download only the relevant portions of a data granule), some of which are called using default values behind the scenes if the user simply skips to the `download_granules` step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Key Steps for Programmatic Data Access\n",
    "\n",
    "There are several key steps for accessing data from the [NASA Harmony API](https://harmony.earthdata.nasa.gov/docs):\n",
    "1. Define your parameters (spatial, temporal, dataset, etc.)\n",
    "2. Query the NASA Harmony API to find out more information about the dataset\n",
    "4. Define additional parameters (e.g. subsetting/customization options)\n",
    "5. Order your data\n",
    "6. Download your data\n",
    "\n",
    "icepyx streamlines this process into a minimal number of lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Login to NASA Earthdata\n",
    "When downloading data from the NASA NSIDC DAAC, all users must login using a valid (free) Earthdata account. The process of authenticating is handled by icepyx by creating and handling the required authentication to interface with the data at the DAAC (including ordering and download). Authentication is completed as login-protected features are accessed. In order to allow icepyx to login for us, we still have to ensure that we have made our Earthdata credentials available for icepyx to find.\n",
    "\n",
    "There are multiple ways to provide your Earthdata credentials via icepyx. Behind the scenes, icepyx is using the [earthaccess library](https://nsidc.github.io/earthaccess/). The [earthaccess documentation](https://earthaccess.readthedocs.io/en/latest/tutorials/getting-started/#auth) automatically tries three primary mechanisms for logging in, all of which are supported by icepyx:\n",
    "- through an interactive, in-notebook login (used below); passwords are not shown plain text with this option\n",
    "- with `EARTHDATA_USERNAME` and `EARTHDATA_PASSWORD` environment variables (these are the same as the ones you might have set for icepyx previously)\n",
    "- with stored credentials in a .netrc file (not recommended for security reasons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Define desired search parameters\n",
    "\n",
    "There are three required inputs, depending on how you want to search for data. Two are required in all cases:\n",
    "- `short_name` = the data product of interest, known as its \"short name\".\n",
    "See [ICESat-2 Products](https://nsidc.org/data/icesat-2/data) for a list of the available data products. Note that Quick Looks products are not currently available.\n",
    "- `spatial_extent` = a region of interest to search within. This can be entered as a bounding box, polygon vertex coordinate pairs, or a polygon geospatial file (ESRI Shapefile (.zip or .shz), kml (.kml), and GeoJSON (*.json or .geojson) are currently supported).\n",
    "    - bounding box: Given in decimal degrees for the lower left longitude, lower left latitude, upper right longitude, and upper right latitude.\n",
    "    - polygon vertices: Given as longitude, latitude coordinate pairs of decimal degrees with the last entry a repeat of the first.\n",
    "    - polygon file: A string containing the full file path and name.\n",
    "    \n",
    "*NOTE: The input keyword for `short_name` was updated in the code from `dataset` to `product` to match common usage.\n",
    "This should not affect users providing positional inputs as demonstrated in this tutorial.*\n",
    "\n",
    "*NOTE: You can submit at most one bounding box or a list of lonlat polygon coordinates per object instance. See the [CMR API documentation](https://cmr.earthdata.nasa.gov/search/site/docs/search/api.html#c-shapefile) for geospatial polygon file requirements.*\n",
    "\n",
    "Then, for all non-gridded products, you must include AT LEAST one of the following inputs (temporal or orbital constraints):\n",
    "- `date_range` = the date range for which you would like to search for results. The following formats are accepted: \n",
    "    - A list of two 'YYYY-MM-DD' strings separated by a comma\n",
    "    - A list of two 'YYYY-DOY' strings separated by a comma\n",
    "    - A list of two datetime.date or datetime.datetime objects\n",
    "    - Dict with the following keys:\n",
    "        - `start_date`: start date; type can be datetime.datetime, datetime.date, or strings (format 'YYYY-MM-DD' or 'YYYY-DOY')\n",
    "        - `end_date`: end date; type can be datetime.datetime, datetime.date, or strings (format 'YYYY-MM-DD' or 'YYYY-DOY')\n",
    "- `cycles` = Which orbital cycle to use, input as a numerical string or a list of strings. If no input is given, this value defaults to all available cycles within the search parameters.  An orbital cycle refers to the 91-day repeat period of the ICESat-2 orbit.\n",
    "- `tracks` = Which [Reference Ground Track (RGT)](https://icesat-2.gsfc.nasa.gov/science/specs) to use, input as a numerical string or a list of strings. If no input is given, this value defaults to all available RGTs within the spatial and temporal search parameters.\n",
    "\n",
    "Below are examples of each type of spatial extent and temporal input and an example using orbital parameters. Please choose and run only one of the input option cells to set your spatial and temporal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bounding box\n",
    "short_name = 'ATL06'\n",
    "spatial_extent = [-55, 68, -48, 71]\n",
    "date_range = ['2019-02-20','2019-02-28']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# polygon vertices (here equivalent to the bounding box, above)\n",
    "short_name = 'ATL06'\n",
    "spatial_extent = [(-55, 68), (-55, 71), (-48, 71), (-48, 68), (-55, 68)]\n",
    "date_range = ['2019-02-20','2019-02-28']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bounding box with 'YYYY-DOY' date range (equivalent to 'YYYY-MM-DD' date ranges above)\n",
    "short_name = 'ATL06'\n",
    "spatial_extent = [-55, 68, -48, 71]\n",
    "date_range = ['2019-051','2019-059']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# polygon vertices with datetime.datetime date ranges\n",
    "import datetime as dt\n",
    "\n",
    "start_dt = dt.datetime(2019, 2, 20, 0, 10, 0)\n",
    "end_dt = dt.datetime(2019, 2, 28, 14, 45, 30)\n",
    "short_name = 'ATL06'\n",
    "spatial_extent = [(-55, 68), (-55, 71), (-48, 71), (-48, 68), (-55, 68)]\n",
    "date_range = [start_dt, end_dt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bounding box with dict containing date ranges\n",
    "short_name = 'ATL06'\n",
    "spatial_extent = [-55, 68, -48, 71]\n",
    "date_range = {\"start_date\": start_dt, \"end_date\": '2019-02-28'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #polygon geospatial file (metadata match but no subset match)\n",
    "# short_name = 'ATL06'\n",
    "# spatial_extent = './supporting_files/data-access_PineIsland/glims_polygons.kml'\n",
    "# date_range = ['2019-02-22','2019-02-28']\n",
    "\n",
    "# #polygon geospatial file (subset and metadata match)\n",
    "# short_name = 'ATL06'\n",
    "# spatial_extent = './supporting_files/data-access_PineIsland/glims_polygons.shp'\n",
    "# date_range = ['2019-10-01','2019-10-05']\n",
    "\n",
    "#polygon geospatial file (same area as other examples; subset and metadata match)\n",
    "short_name = 'ATL06'\n",
    "spatial_extent = './supporting_files/simple_test_poly.gpkg'\n",
    "date_range = ['2019-10-01','2019-10-05']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Create the data object using inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "region_a = ipx.Query(short_name, spatial_extent, date_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# using orbital parameters with one of the above data products + spatial parameters\n",
    "region_a = ipx.Query(short_name, spatial_extent,\n",
    "   cycles=['03','04','05','06','07'], tracks=['0849','0902'])\n",
    "\n",
    "print(region_a.product)\n",
    "print(region_a.product_version)\n",
    "print(region_a.cycles)\n",
    "print(region_a.tracks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "These properties include visualization of the spatial extent on a map. The style of map you will see depends on whether or not you have a certain library, `geoviews`, installed. Under the hood, this is because the `proj` library must be installed with conda (it is not available from PyPI) to support some `geoviews` dependencies. With `geoviews`, this plotting function returns an interactive map. Otherwise, your spatial extent will plot on a static map using `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(region_a.spatial_extent)\n",
    "region_a.visualize_spatial_extent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Formatted parameters and function calls allow us to see the the properties of the data object we have created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(region_a.product)\n",
    "print(region_a.temporal) # .dates, .start_time, .end_time can also be used for a piece of this information\n",
    "# print(region_a.dates)\n",
    "# print(region_a.start_time)\n",
    "# print(region_a.end_time)\n",
    "print(region_a.cycles)\n",
    "print(region_a.tracks)\n",
    "print(region_a.product_version)\n",
    "region_a.visualize_spatial_extent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "There are also several optional inputs to allow the user finer control over their search. Start and end time are only valid inputs on a temporally limited search, and they are ignored if your `date_range` input is a datetime.datetime object.\n",
    "- `start_time` = start time to search for data on the start date. If no input is given, this defaults to 00:00:00.\n",
    "- `end_time` = end time for the end date of the temporal search parameter. If no input is given, this defaults to 23:59:59. \n",
    "\n",
    "Times must be input as 'HH:mm:ss' strings or datetime.time objects.\n",
    "\n",
    "- `version` = What version of the data product to use, input as a numerical string. If no input is given, this value defaults to the most recent version of the product specified in `short_name`.\n",
    "\n",
    "*NOTE Version 002 is used as an example in the below cell. However, using it will cause 'no results' errors in granule ordering for some search parameters. These issues have been resolved in later versions of the data products, so it is best to use the most recent version where possible.\n",
    "Similarly, if you try to order/download a retired version no longer accessible at the NASA NSIDC DAAC, you will get a \"no data matched your request\" error.\n",
    "Thus, you will need to update the version associated with `region_a` and rerun the next cell for the rest of this notebook to run.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "region_a = ipx.Query(short_name, spatial_extent, date_range, \\\n",
    "   start_time='03:30:00', end_time='21:30:00', version='002')\n",
    "\n",
    "print(region_a.product)\n",
    "print(region_a.dates)\n",
    "print(region_a.product_version)\n",
    "print(region_a.spatial)\n",
    "print(region_a.temporal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Alternatively, you can also just create the query object without creating named variables first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "region_a = ipx.Query('ATL06',[-55, 68, -48, 71],\n",
    "    ['2019-02-01','2019-02-28'], start_time='00:00:00', end_time='23:59:59')\n",
    "\n",
    "print(region_a.product)\n",
    "print(region_a.dates)\n",
    "print(region_a.product_version)\n",
    "print(region_a.spatial)\n",
    "print(region_a.temporal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### More information about your query object\n",
    "In addition to viewing the stored object information shown above (e.g., product short name, start and end date and time, version), we can also request summary information about the data product itself or confirm that we have manually specified the latest version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "region_a.product_summary_info()\n",
    "print(region_a.latest_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "If the summary does not provide all of the information you are looking for, or you would like to see information for previous versions of the data product, all available metadata for the collection product is available in a readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "region_a.product_all_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Querying a data product\n",
    "In order to search the product collection for available data granules, we need to build our search parameters. This is done automatically behind the scenes when you run `region_a.avail_granules()`, but you can also build and view them by calling `region_a.CMRparams`. These are formatted as a dictionary of key-value pairs according to the [CMR documentation](https://cmr.earthdata.nasa.gov/search/site/docs/search/api.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#build and view the parameters that will be submitted in our query\n",
    "region_a.CMRparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Now that our parameter dictionary is constructed, we can search the CMR database for the available granules.\n",
    "Granules returned by the CMR metadata search are automatically stored within the data object.\n",
    "The search completed at this level relies completely on the granules' metadata.\n",
    "As a result, some (and in rare cases all) of the granules returned may not actually contain data in your specified region, particularly if the region is small or located near the boundaries of a given granule. If this is the case, the subsetter will not return any data when you actually place the order.\n",
    "A warning message will be included in the order.status output for each granule to which this applies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#search for available granules and provide basic summary info about them\n",
    "region_a.avail_granules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get a list of granule IDs for the available granules\n",
    "region_a.avail_granules(ids=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsetting\n",
    "\n",
    "For a deeper dive into subsetting, please see our [Subsetting Tutorial Notebook](https://icepyx.readthedocs.io/en/latest/example_notebooks/IS2_data_access2-subsetting.html), which covers subsetting in more detail.\n",
    "\n",
    "Subsetting utilizes the NASA Harmony subsetting service to spatially and temporally extract the data you are interested in. The advantages of using Harmony include:\n",
    "* easily reproducible downloads, particularly when coupled with an icepyx query object\n",
    "* smaller file size, meaning faster downloads and less storage required\n",
    "* easily order more data with the same or similar search parameters\n",
    "* quickly move to analysis and navigate your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Place the order\n",
    "Then, we can send the order to Harmony using the order_granules function. Information about the granules ordered and their status will be printed automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "order = region_a.order_granules()\n",
    "# region_a.order_granules(subset=False)\n",
    "order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "order.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the order\n",
    "Finally, we can download our order to a specified directory (which needs to have a full path but doesn't have to point to an existing directory) and the download status will be printed as the program runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = './download'\n",
    "region_a.download_granules(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Credits**\n",
    "* original notebook by: Jessica Scheick\n",
    "* notebook contributors: Amy Steiker, Tyler Sutterley, and Theresa Andersen\n",
    "* source material: [NSIDC Data Access Notebook](https://github.com/ICESAT-2HackWeek/ICESat2_hackweek_tutorials/tree/master/03_NSIDCDataAccess_Steiker) by Amy Steiker and Bruce Wallin and [2020 Hackweek Data Access Notebook](https://github.com/ICESAT-2HackWeek/2020_ICESat-2_Hackweek_Tutorials/blob/main/06-07.Data_Access/02-Data_Access_rendered.ipynb) by Jessica Scheick and Amy Steiker"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
